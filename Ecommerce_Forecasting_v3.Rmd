---
title: "TimeSeries_Ecommerce_Forecasting"
author: "jennifer siwu"
date: "7/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results="hide",warning=FALSE, message=FALSE, include=FALSE}
library(lubridate)
library(forecast)

library(tidyverse)
library(fable)
library(tsibble)
library(fabletools)
library(feasts)
library(prophet)

library(ggplot2)
library(ggfortify)
library(aTSA)
```

# Reading the data

The dataset came from 3 different tables: orders, order items, and items. 
From the order table, we need to get the dates based on the order purchase date because in demand planning perspective, the order starts from that point. In other words, in demand planning realm, we should have the inventory based on the customer purchase date.
```{r}
ord_df <- read.csv("~/Desktop/MSBA/Winter 2020/Bus Data Mgmt/brazilian-ecommerce/olist_orders_dataset.csv", header=TRUE)
oi_df <- read.csv("~/Desktop/MSBA/Winter 2020/Bus Data Mgmt/brazilian-ecommerce/olist_order_items_dataset.csv", header=TRUE)
itm_df <- read.csv("~/Desktop/MSBA/Winter 2020/Bus Data Mgmt/brazilian-ecommerce/olist_products_dataset.csv", header=TRUE)
itm_eng_df <- read.csv("~/Desktop/MSBA/Winter 2020/Bus Data Mgmt/brazilian-ecommerce/product_category_name_translation.csv", header=TRUE)
```

```{r}
#str(ord_df)
#str(oi_df)
#str(itm_df)

head(ord_df)
head(oi_df)
head(itm_df)
head(itm_eng_df)
```


# Merging The data

First, Let's combine the product name and category name translation together.
```{r}
product_complete <- left_join(itm_df, itm_eng_df, by = "product_category_name", copy = FALSE)

items_df <- 
  product_complete  %>% dplyr::select(product_id,
           product_category_name,
           product_category_name_english) %>%
               mutate(product_category_name_english =ifelse(is.null(product_category_name_english) 
                                                            & product_category_name == "pc_gamer","pc_gamer",
                                                      ifelse(is.null(product_category_name_english) 
                                                             & product_category_name != "pc_gamer","portable_mixer",
                                                             product_category_name_english)
                                                      )) %>%
                                                      dplyr::select(product_id,product_category_name_english)
    
head(items_df)
```


Now, let's create the master datafile as a combination of all 3 tables (ord_df, oi_df, and items_df)
```{r, warning=FALSE, message=FALSE}
order <- dplyr::select(ord_df, c(order_id,order_purchase_timestamp)) %>%
          mutate(order_purchase_timestamp=as.Date(order_purchase_timestamp))
order <- order[order$order_purchase_timestamp >= as.Date('2017-01-01') & order$order_purchase_timestamp < as.Date('2018-08-01'),]


orderitem <- oi_df %>%
              dplyr::select(order_id,order_item_id,product_id,price) %>%
              mutate(total_price = order_item_id*price,
                     qty  = order_item_id) %>% dplyr::select(-order_item_id)

master_df <- inner_join(inner_join(order,orderitem, by = "order_id", copy = FALSE),items_df, by = "product_id", copy = FALSE)
head(master_df)
```


```{r, warning=FALSE, message=FALSE}
tsdf <- master_df %>% 
            group_by(order_purchase_timestamp,product_category_name_english) %>%
            summarise(sales_volume = sum(qty),
                      revenue = sum(total_price))

top_sales <- tsdf %>% group_by(product_category_name_english) %>% 
              summarise(sales_volume = sum(sales_volume)) %>%  arrange(desc(sales_volume)) %>% 
                    mutate(cummulative_sales = cumsum(sales_volume)/sum(tsdf$sales_volume))

top_rev <- tsdf %>% group_by(product_category_name_english) %>% 
              summarise(revenue = sum(revenue)) %>%  arrange(desc(revenue)) %>% 
                    mutate(cummulative_revenue = cumsum(revenue)/sum(tsdf$revenue))

top80sales <- top_sales[top_sales$cummulative_sales <= .8,] %>% arrange(cummulative_sales)
top80revenue <- top_rev[top_rev$cummulative_revenue <= .8,]


ggplot(top80sales, aes(x=reorder(product_category_name_english,-cummulative_sales))) +
  geom_bar(aes(y=sales_volume), fill='#06d997', stat="identity") +
  theme(axis.text.x = element_text(angle=90, vjust=0.6)) +
  labs(title = "Top 80% Cummulative Sales", subtitle = "Sales Volume by Product Category"
       , x = 'Product Category', y ='Sales (in Units)') + coord_flip()

ggplot(top80revenue, aes(x=reorder(product_category_name_english,-cummulative_revenue))) +
  geom_bar(aes(y=revenue/10000), fill='#213ee2', stat="identity") +
  theme(axis.text.x = element_text(angle=90, vjust=0.6)) +
  labs(title = "Top 80% Cummulative Revenue", subtitle = "Revenue by Product Category", 
       x = 'Product Category', y ='Revenue (in 10K)')+  coord_flip()
```


# Looking at the overall time series trend
There seems to be an extreme jump on November 24th in 2017 which is a black friday. The fact that the sales was 3x of regular daily sales, and only happened in one day of the year makes it really difficult to forecast. However, this scenario actually is very common for any retail/e-commerce businesses. This is referred as "Peak Seasons". The range of **Peak Season** for ecomm or retail businesses usually start from Halloween all the way to Post New Years, with many days within those timeframe having very extreme sales. Based on the decomposition, we can see some upward trends.
```{r , warning=FALSE, message=FALSE}
tsdf2 <- master_df  %>% filter(product_category_name_english %in% top80sales$product_category_name_english) %>% 
            group_by(order_purchase_timestamp) %>% 
                summarise(sales_volume = sum(qty))
#ts(tsdf2$sales_volume, start = c(2016, 9, 4)) %>% autoplot()
p <- ggplot(tsdf2, aes(x=order_purchase_timestamp, y=sales_volume)) +
  geom_line(color = "#213ee2") + 
  ggtitle("Top 80% Sales Volume (2017 to 2018)") +
  theme(plot.title = element_text(size = 22, face = "bold"))
p
```


```{r, warning=FALSE}
dc1_classical_add <- decompose(ts(tsdf2$sales_volume, frequency = 7), type = "additive")
autoplot(dc1_classical_add) +
  ggtitle("Top 80% Sales Volume Classical Additive Decomposition") + 
  xlab("order_purchase_timestamp")
```

# Time Series for Top 80% Sales Product Categories

Let's zoom in on the time series a little bit to see the jump in black friday more closely based on each product category.
Below is the time series plot for the top 3 product category. As we can see below, each product categories have different seasonality and they have different peak effect. With this analysis, I decided to do a bottom up forecast, meaning that I will create different models per each product category and sum it up at the end for the overall forecast. As for the items in each product category, I will allocate the forecast based on the their % of sales within the category, this is due to the idea that each product within the same category will more or less follow the same seasonality and trend.

For this analysis, I will focus only on the product categories that make up the top 80% Sales (~11M in Average Annual Revenue). 
```{r}
tsdftemp <- tsdf[tsdf$product_category_name_english %in% top80sales$product_category_name_english,] %>% 
  mutate(total_revenue = revenue*sales_volume) 

sum(tsdftemp$total_revenue)/nrow(tsdftemp)*365
```

```{r}
tsdf2 <- select(tsdf,-revenue)[tsdf$product_category_name_english %in% c("bed_bath_table","furniture_decor"
                                                                         ,"health_beauty"),]

p <- ggplot(tsdf2[tsdf2$order_purchase_timestamp>= as.Date("2017-10-01"),], aes(x=order_purchase_timestamp, y=sales_volume, color = product_category_name_english)) +
  geom_line() + 
  ggtitle("Sales Volume from Oct 2017 to Sep 2018") +
  scale_x_date(date_labels = "%m-%Y") +
  scale_color_manual(values=c("#213ee2", "#06d997","#a49cf8")) +
  theme(plot.title = element_text(size = 22, face = "bold"))
p
```


```{r, warning=FALSE, message=FALSE}
tsdf80 <- tsdf[tsdf$product_category_name_english %in% top80sales$product_category_name_english,]
tsdf80f <- tsdf80 %>% mutate(dow = wday(order_purchase_timestamp, label=TRUE),
                             month = month(order_purchase_timestamp, label=TRUE),
                              day = day(order_purchase_timestamp)) 


monthtemp <- tsdf80f %>% group_by(month) %>% summarise(sales_volume= sum(sales_volume)) 

temp <- tsdf80f %>% group_by(dow) %>% summarise(sales_volume= sum(sales_volume)) 
temp$dow <- ordered(temp$dow, levels=c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))

daytemp <- tsdf80f %>% group_by(day) %>% summarise(sales_volume= sum(sales_volume)) 


ggplot(monthtemp, aes(x=month, y=sales_volume, group = 1)) +
  geom_line(color = "#213ee2",size=1.5) + 
  geom_point(color = "#06d997",size=5)+
  ggtitle("Yearly Trend") +
  theme(plot.title = element_text(size = 22, face = "bold"))

ggplot(daytemp, aes(x=day, y=sales_volume, group = 1)) +
  geom_line(color = "#043e83",size=1) + 
  geom_point(color = "#a49cf8",size=4)+
  ggtitle("Monthly Trend") +
  theme(plot.title = element_text(size = 22, face = "bold"))

ggplot(temp, aes(x=dow, y=sales_volume,fill=dow)) +
  geom_bar(position="dodge", stat="identity") + 
  ggtitle("Weekly Trend") +
  scale_fill_manual(values=c("#213ee2","#a49cf8","#5071dd","#88b0e4","#06d997","#046a41","#043e83")) +
  theme(plot.title = element_text(size = 22, face = "bold"))
```


# Forecasting for Overall to Compare Models
Training/Testing Split = I will use the last 4 months data (April 2018-July 2018) as a testing set, while the remaining previous data will be used to train the forecasting models.

## Arima, Sarima, Sarimax

```{r, message=FALSE}
#PACF & ACF showing so many lags correlated with the last values -->  Need to make data stationary
tsreg <- tsdf80f %>% group_by(order_purchase_timestamp) %>% summarise(sales_volume=sum(sales_volume))
tsregts <-  ts(tsreg$sales_volume, frequency = 7)
autoplot(tsregts, ts.colour = "#213ee2") 
Acf(tsregts, lag=60)
Pacf(tsregts, lag=60)

#using diff to make data stationary --> already looking much better
tsdiff <- tsdf80f %>% group_by(order_purchase_timestamp) %>% summarise(sales_volume=sum(sales_volume))
tsdiffts <-  ts(tsdiff$sales_volume, frequency = 7)
diff1 <- diff(tsdiffts, differences = 7)
autoplot(diff1, ts.colour = "#213ee2") 
Acf(diff1, lag=60)
Pacf(diff1, lag=60)
adf.test(diff1, nlag = 1)

```


## Training Arima Model
Training cutoff is March 1st 2018 since we cant allow irregular cutoff for arima.
```{r, warning = FALSE, message = FALSE}
tempdf <- tsdf80f %>% group_by(order_purchase_timestamp,day,month,dow) %>% summarise(sales_volume=sum(sales_volume))
#adding the holiday calendar information
hldy <- read.csv("~/Documents/ecommerce_sales_forecasting/holidaycalendarbrazil.csv", header=TRUE) %>% mutate(date = as.Date(as.character(Date), format = "%m/%d/%y"))

month <- as.character(tempdf$month)
dow <- as.character(tempdf$dow)
monthdum <- as.data.frame(model.matrix(~month-1))[,2:12]
dowdum <-as.data.frame(model.matrix(~dow-1))[,2:7]

tempdf <- tempdf %>% mutate(weekday =  ifelse(as.character( wday(order_purchase_timestamp, label=TRUE)) %in% c("Sun","Sat"), 0,1))
tempdf1 <- cbind(tempdf,monthdum,dowdum)

#merging the holiday information to the ts dataframe
merged_df <- left_join(tempdf1, hldy, by = c("order_purchase_timestamp"="date"), copy = FALSE)
merged_df['holiday']<- ifelse(is.na(merged_df$Date),0,1)

merged_df <- merged_df %>% select(-Date)
merged_df$product_category <- "all"

alltsb <- as_tsibble(merged_df, index = order_purchase_timestamp, key = product_category)
train <- merged_df %>% filter(order_purchase_timestamp < '2018-05-01')
test <- merged_df %>% filter(order_purchase_timestamp >= '2018-05-01')
traintsb <- as_tsibble(train, index = order_purchase_timestamp, key = product_category)
testtsb <- as_tsibble(test, index = order_purchase_timestamp, key = product_category)
```

### Analyzing individual models
The box test and residual analysis shows that the tuned Sarima follows correct assumptions and also is a better model than the auto arima. 
The manual/tuned Arima model shows better result since it is based on a thorough analysis on the PACF and ACF not just basing it off the log likelihood. 

Additionally, the residual diagnostic indicated that the residuals are white noise, therefore the forecast is generally good. The Box Test results are not significant (i.e., the p-values are relatively large). Thus, we can conclude that the residuals are not distinguishable from a white noise series.


```{r}
traints <- ts(train$sales_volume, frequency = 7)
testts <- ts(test$sales_volume, frequency = 7)
ar_man <- arima(traints, order=c(4,1,2), seasonal= list(order=c(0,1,2), period=7))
ar_au <- auto.arima(traints)

print("Tuned Arima")
summary(ar_man)
Box.test(residuals(ar_man))

print("Auto Arima")
summary(ar_au)
Box.test(residuals(ar_au))

checkresiduals((ar_man))

```



